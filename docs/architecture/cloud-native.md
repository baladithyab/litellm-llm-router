# LiteLLM Cloud-Native Enhancements Backlog

> **Attribution**:
> RouteIQ is built on top of upstream [LiteLLM](https://github.com/BerriAI/litellm) for proxy/API compatibility and [LLMRouter](https://github.com/ulab-uiuc/LLMRouter) for ML routing.

This repository serves as a **production-grade enhancement layer** on top of the upstream [LiteLLM Proxy](https://github.com/BerriAI/litellm). While LiteLLM provides the core unified API gateway, this layer adds the necessary "glue" for robust, cloud-native, and air-gapped ("moat-mode") deployments.

## Philosophy: Enhancement, Not Fork

We do **not** fork LiteLLM. All enhancements are implemented via:
1.  **Middleware/Callbacks**: Intercepting requests/responses for routing, logging, and security.
2.  **Sidecars/Deploy Overlays**: Helm charts, Docker Compose, and sidecar containers (e.g., for config sync).
3.  **Documentation**: Runbooks and architectural patterns for enterprise deployment.

## Capability Matrix

| Feature Area | Upstream LiteLLM Provides | This Enhancement Layer Adds |
| :--- | :--- | :--- |
| **Core Proxy** | Unified API (OpenAI compatible), 100+ providers. | ML-based Routing ([`llmrouter`](ml-routing-cloud-native.md)), A2A/MCP protocol support. |
| **Deployment** | Basic Docker image, Helm chart. | Hot-reload config sync, streaming-aware shutdown, air-gapped "moat-mode" patterns. |
| **Observability** | Prometheus metrics, OTel traces, logging callbacks. | Multi-replica trace correlation, routing decision visibility, durable audit export. |
| **Resilience** | Retries, fallbacks, simple rate limiting. | Advanced backpressure, load shedding, degraded mode operation. |
| **Security** | API Keys, RBAC, Secret Managers. | Hardened "moat-mode" configs, mTLS guidance, secret rotation patterns. |

---

## Enhancement Backlog

### A. HA + Deployment + Config Sync

| Priority | Item | Rationale | Approach (Cloud-Agnostic) | Implementation Surface |
| :--- | :--- | :--- | :--- | :--- |
| **P0** | **Hot-Reload Config Sync** | Changing config/models shouldn't require restart (downtime). | Sidecar watches S3/MinIO/GCS bucket (ETag) -> downloads -> triggers SIGHUP or API reload. | Code (`ConfigSyncManager`) + Sidecar Container |
| **P1** | **Streaming-Aware Shutdown** | SIGTERM currently kills active streams immediately. | Intercept SIGTERM -> stop accepting new conns -> wait for active streams to finish (with timeout) -> exit. | Middleware / Signal Handler |
| **P2** | **Autoscaling Guidance** | CPU/Memory isn't enough for LLM scaling (token throughput matters). | Publish custom metrics (active_streams, token_throughput) for HPA/KEDA to consume. | Docs + Helm Chart (KEDA ScaledObject) |
| **P2** | **Multi-Region Sync** | HA across regions requires eventual consistency for configs. | Use object storage replication (e.g., S3 CRR) + local sync agents. | Docs / Architecture Pattern |

### B. Security / Tenancy / Secrets

| Priority | Item | Rationale | Approach (Cloud-Agnostic) | Implementation Surface |
| :--- | :--- | :--- | :--- | :--- |
| **P1** | **Secret Rotation Patterns** | Long-lived API keys are a risk. | Support dynamic reloading of secrets from file mounts (K8s Secrets/Vault Agent) without restart. | Docs + Config Reload Logic |
| **P2** | **Strict Tenancy Isolation** | Prevent "noisy neighbor" effect between teams. | Enforce per-team concurrency limits (bulkheads) in addition to rate limits. | Middleware (Custom Guardrail) |
| **P2** | **Moat-Mode Hardening** | Air-gapped envs have strict egress rules. | Validate "no-internet" startup; pre-download all tokenizer assets; local-only fallback config. | Docker Build (Vendoring) + Docs |

### C. Reliability / Performance

| Priority | Item | Rationale | Approach (Cloud-Agnostic) | Implementation Surface |
| :--- | :--- | :--- | :--- | :--- |
| **P1** | **Backpressure / Load Shedding** | Overload causes cascading failures. | Reject requests immediately (HTTP 503) if queue depth > X or latency > Y. | Middleware |
| **P2** | **Degraded Mode** | If DB/Redis fails, proxy should still serve basic requests. | Circuit breaker: if Redis down, switch to in-memory cache/limit; if DB down, use local fallback keys. | Code (Resilience Logic) |
| **P2** | **Jittered Retries** | Thundering herd after outage. | Standardize exponential backoff + jitter across all clients/internal retries. | Docs (Client Guidance) + Config |

### D. Observability / Ops

| Priority | Item | Rationale | Approach (Cloud-Agnostic) | Implementation Surface |
| :--- | :--- | :--- | :--- | :--- |
| **P0** | **Routing Decision Visibility** | "Why did the router pick model X?" is a black box. | Add OTel attributes: `router.strategy`, `router.score`, `router.candidates`. | Code (Router Instrumentation) |
| **P1** | **Multi-Replica Correlation** | Tracing requests across Nginx -> Gateway -> DB is hard. | Propagate `traceparent` / `b3` headers; ensure structured logs include `trace_id`. | Middleware + Logging Config |
| **P1** | **Durable Audit Export** | Compliance requires non-repudiable logs. | Async write of audit logs to S3/GCS (batching) or Kafka/Redpanda. | Background Task / Callback |

---

## Execution Plan

### Phase 1: Foundation (P0 Items)
*Focus: Zero-downtime updates and basic visibility.*
1.  Implement **Hot-Reload Config Sync** sidecar/thread.
2.  Instrument **Routing Decision Visibility** in `llmrouter`.
3.  Validate with `docs/VALIDATION_PLAN.md` (Hot Reload Test).

### Phase 2: Resilience & Security (P1 Items)
*Focus: Production hardening.*
1.  Implement **Streaming-Aware Shutdown**.
2.  Add **Backpressure** middleware.
3.  Enhance **Multi-Replica Correlation** (header propagation).
4.  Document **Secret Rotation** patterns.

### Phase 3: Scale & Advanced Ops (P2 Items)
*Focus: High scale and strict compliance.*
1.  Create **Autoscaling Guidance** (KEDA/HPA).
2.  Implement **Degraded Mode** circuit breakers.
3.  Finalize **Moat-Mode Hardening** artifacts.

## Validation

All enhancements must be validated against the [Validation Plan](../VALIDATION_PLAN.md):
-   **Hot Reload**: Verify config updates propagate within `reload_interval` without dropping requests.
-   **Shutdown**: Verify active streams complete during SIGTERM.
-   **Observability**: Verify traces contain routing metadata in Jaeger/Tempo.

## 2. ML Routing Architecture

We introduce a new `LLMRouter` component that sits between the API layer and the LiteLLM proxy.

See [ML Routing Architecture](mlops-loop.md) for the detailed design.

## 3. Validation Plan

We have established a comprehensive validation plan to ensure feature parity and stability.

See [Validation Plan](../../plans/validation-plan.md).
