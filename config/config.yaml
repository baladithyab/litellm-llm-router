# LiteLLM + LLMRouter Configuration
# =============================================================================
# This is the main configuration file for the LiteLLM gateway with LLMRouter
# intelligent routing strategies.

# =============================================================================
# Model List - Define your LLM providers and models
# =============================================================================
model_list:
  # Anthropic Models (Direct API)
  - model_name: claude-3-opus
    litellm_params:
      model: anthropic/claude-3-opus-20240229
      api_key: os.environ/ANTHROPIC_API_KEY

  - model_name: claude-3-sonnet
    litellm_params:
      model: anthropic/claude-3-sonnet-20240229
      api_key: os.environ/ANTHROPIC_API_KEY

  - model_name: claude-3-haiku
    litellm_params:
      model: anthropic/claude-3-haiku-20240307
      api_key: os.environ/ANTHROPIC_API_KEY

  # AWS Bedrock Models (matches trained router models)
  - model_name: claude-haiku
    litellm_params:
      model: bedrock/anthropic.claude-3-haiku-20240307-v1:0
      aws_region_name: os.environ/AWS_REGION

  - model_name: claude-sonnet
    litellm_params:
      model: bedrock/anthropic.claude-3-sonnet-20240229-v1:0
      aws_region_name: os.environ/AWS_REGION

  - model_name: claude-opus
    litellm_params:
      model: bedrock/anthropic.claude-3-opus-20240229-v1:0
      aws_region_name: os.environ/AWS_REGION

# =============================================================================
# Router Settings - LLMRouter Integration
# =============================================================================
router_settings:
  # Routing strategy options:
  # LiteLLM built-in: simple-shuffle, least-busy, latency-based-routing,
  #                   cost-based-routing, usage-based-routing
  # LLMRouter ML-based: llmrouter-knn, llmrouter-svm, llmrouter-mlp,
  #                     llmrouter-mf, llmrouter-elo, llmrouter-hybrid,
  #                     llmrouter-custom
  routing_strategy: llmrouter-knn

  # LLMRouter strategy arguments (used when routing_strategy starts with 'llmrouter-')
  routing_strategy_args:
    # Path to trained model directory (trained via examples/mlops pipeline)
    model_path: /app/models/knn_router
    # Path to LLM candidates JSON
    llm_data_path: /app/config/llm_candidates.json
    # Enable hot-reloading of model
    hot_reload: true
    # How often to check for model updates (seconds)
    reload_interval: 300
    # S3 settings for remote model loading (optional)
    # model_s3_bucket: my-bucket
    # model_s3_key: models/knn_router/

  # Retry/fallback settings
  num_retries: 2
  retry_after: 5
  timeout: 600

  # Enable routing caching
  cache_responses: true

# =============================================================================
# General Settings
# =============================================================================
general_settings:
  # Master key for admin access
  master_key: os.environ/LITELLM_MASTER_KEY

  # Database for persistence (optional)
  database_url: os.environ/DATABASE_URL

  # Disable storing prompts in DB for privacy
  store_model_in_db: true

# =============================================================================
# LiteLLM Settings
# =============================================================================
litellm_settings:
  # Enable response caching
  cache: true
  cache_params:
    type: redis
    host: os.environ/REDIS_HOST
    port: os.environ/REDIS_PORT
    ttl: 3600

  # Logging
  set_verbose: false

  # Cost tracking
  success_callback: ["langfuse"]
  failure_callback: ["langfuse"]

  # Rate limiting (optional)
  # max_budget: 100
  # budget_duration: 30d

# =============================================================================
# MCP Servers - Model Context Protocol (optional)
# =============================================================================
# Uncomment and configure to enable MCP tools
# mcp_servers:
#   # HTTP-based MCP server
#   my_mcp_server:
#     url: "http://localhost:8080/mcp"
#     transport: "streamable_http"  # or "sse", "stdio"
#
#   # OpenAPI-to-MCP conversion
#   petstore_api:
#     url: "https://petstore.swagger.io/v2"
#     spec_path: "/path/to/openapi.json"
#     auth_type: "none"  # or "api_key", "bearer_token", "oauth2"

# =============================================================================
# A2A Agents - Agent-to-Agent Protocol (optional)
# =============================================================================
# Agents are typically added via the UI or API, but can also be pre-configured
# See docs: https://docs.litellm.ai/docs/a2a

# =============================================================================
# Environment Variables Reference
# =============================================================================
# Required:
#   - LITELLM_MASTER_KEY: Master API key for admin access
#   - OPENAI_API_KEY: OpenAI API key
#   - ANTHROPIC_API_KEY: Anthropic API key
#
# Optional:
#   - DATABASE_URL: PostgreSQL connection string
#   - REDIS_HOST: Redis host for caching
#   - REDIS_PORT: Redis port (default: 6379)
#   - AZURE_API_KEY: Azure OpenAI API key
#   - AZURE_API_BASE: Azure OpenAI endpoint
#
# Gateway Features:
#   - A2A_GATEWAY_ENABLED: Enable A2A agent gateway (default: false)
#   - MCP_GATEWAY_ENABLED: Enable MCP server gateway (default: false)
#   - STORE_MODEL_IN_DB: Store models/MCPs in database (default: false)
#
# Config Sync & Hot Reload:
#   - CONFIG_S3_BUCKET: S3 bucket for config sync
#   - CONFIG_S3_KEY: S3 key for config file
#   - CONFIG_HOT_RELOAD: Enable hot reload on config changes (default: false)
#   - CONFIG_SYNC_INTERVAL: Sync interval in seconds (default: 60)
