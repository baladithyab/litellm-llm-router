# LiteLLM + LLMRouter - Local Test Configuration
# Tests all features: A2A, MCP, Routing Strategies, Hot Reload
# Uses AWS Bedrock models (Claude 4.5 family + Nova 2 family)

# =============================================================================
# Model List - AWS Bedrock Models (IAM Profile auth)
# Uses Cross-Region Inference Profiles for Claude models
# =============================================================================
model_list:
  # ---------------------------------------------------------------------------
  # Claude 4.5 Family (Anthropic via Bedrock - Cross-Region Inference)
  # Uses us. prefix for cross-region inference profiles
  # ---------------------------------------------------------------------------
  - model_name: claude-4.5-sonnet
    litellm_params:
      model: bedrock/us.anthropic.claude-sonnet-4-5-20250929-v1:0
      aws_region_name: us-east-1
    model_info:
      id: claude-4.5-sonnet
      mode: chat
      description: "Claude 4.5 Sonnet - Latest and most capable Sonnet model"

  - model_name: claude-4.5-opus
    litellm_params:
      model: bedrock/us.anthropic.claude-opus-4-5-20251101-v1:0
      aws_region_name: us-east-1
    model_info:
      id: claude-4.5-opus
      mode: chat
      description: "Claude 4.5 Opus - Highest capability for complex tasks"

  - model_name: claude-4.5-haiku
    litellm_params:
      model: bedrock/us.anthropic.claude-haiku-4-5-20251001-v1:0
      aws_region_name: us-east-1
    model_info:
      id: claude-4.5-haiku
      mode: chat
      description: "Claude 4.5 Haiku - Fast and cost-effective"

  # Claude 4 Sonnet (previous generation - cross-region)
  - model_name: claude-4-sonnet
    litellm_params:
      model: bedrock/us.anthropic.claude-sonnet-4-20250514-v1:0
      aws_region_name: us-east-1
    model_info:
      id: claude-4-sonnet
      mode: chat
      description: "Claude 4 Sonnet - Balanced performance"

  # ---------------------------------------------------------------------------
  # Amazon Nova Family (on-demand available)
  # ---------------------------------------------------------------------------
  - model_name: nova-pro
    litellm_params:
      model: bedrock/amazon.nova-pro-v1:0
      aws_region_name: us-east-1
    model_info:
      id: nova-pro
      mode: chat
      description: "Nova Pro - Highly capable multimodal model"

  - model_name: nova-lite
    litellm_params:
      model: bedrock/amazon.nova-lite-v1:0
      aws_region_name: us-east-1
    model_info:
      id: nova-lite
      mode: chat
      description: "Nova Lite - Fast multimodal processing"

  - model_name: nova-micro
    litellm_params:
      model: bedrock/amazon.nova-micro-v1:0
      aws_region_name: us-east-1
    model_info:
      id: nova-micro
      mode: chat
      description: "Nova Micro - Ultra-fast text-only model"

  # ---------------------------------------------------------------------------
  # Embedding Models
  # ---------------------------------------------------------------------------
  - model_name: titan-embed-text-v2
    litellm_params:
      model: bedrock/amazon.titan-embed-text-v2:0
      aws_region_name: us-east-1
    model_info:
      id: titan-embed-text-v2
      mode: embedding
      description: "Titan Text Embeddings V2 - High quality text embeddings"

  - model_name: titan-embed-image-v1
    litellm_params:
      model: bedrock/amazon.titan-embed-image-v1:0
      aws_region_name: us-east-1
    model_info:
      id: titan-embed-image-v1
      mode: embedding
      description: "Titan Multimodal Embeddings - Text and image embeddings"

  # ---------------------------------------------------------------------------
  # Image Generation Models
  # ---------------------------------------------------------------------------
  - model_name: nova-canvas
    litellm_params:
      model: bedrock/amazon.nova-canvas-v1:0
      aws_region_name: us-east-1
    model_info:
      id: nova-canvas
      mode: image_generation
      description: "Nova Canvas - High quality image generation"

  - model_name: titan-image-generator
    litellm_params:
      model: bedrock/amazon.titan-image-generator-v2:0
      aws_region_name: us-east-1
    model_info:
      id: titan-image-generator
      mode: image_generation
      description: "Titan Image Generator V2 - Versatile image generation"

# =============================================================================
# Router Settings - Test Multiple Strategies
# =============================================================================
router_settings:
  # Use LLMRouter KNN strategy for trained model-based routing
  routing_strategy: llmrouter-knn

  # LLMRouter strategy arguments
  routing_strategy_args:
    # Path to the trained KNN router model (.pkl file or directory containing .pkl)
    # Must be a FILE not a directory for proper mtime-based hot reload
    model_path: /app/models/knn_router.pkl
    # Path to LLM candidates JSON (model metadata for routing decisions)
    llm_data_path: /app/config/llm_candidates.json
    # Enable hot reload: gateway watches model_path for changes
    hot_reload: true
    # Reload interval in seconds (check for model updates every 10s for demo)
    reload_interval: 10
    # Embedding model for runtime inference (must match training pipeline)
    # Default: sentence-transformers/all-MiniLM-L6-v2
    embedding_model: sentence-transformers/all-MiniLM-L6-v2
    # Device for embedding model ('cpu' or 'cuda')
    embedding_device: cpu
    # Optional: mapping from predicted labels to LLM candidate keys
    # label_mapping:
    #   model-label-1: claude-4.5-sonnet
    #   model-label-2: nova-pro

  num_retries: 2
  retry_after: 5
  timeout: 300
  cache_responses: true

# =============================================================================
# General Settings
# =============================================================================
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  # Enable database for full feature testing
  database_url: os.environ/DATABASE_URL
  store_model_in_db: true
  # Enable detailed logging for testing
  # alerting: ["slack"]

# =============================================================================
# LiteLLM Settings
# =============================================================================
litellm_settings:
  cache: true
  cache_params:
    type: redis
    host: os.environ/REDIS_HOST
    port: os.environ/REDIS_PORT
    ttl: 300
  set_verbose: true
  drop_params: true
  # OpenTelemetry callbacks for Jaeger tracing
  success_callback: ["otel"]
  failure_callback: ["otel"]

# =============================================================================
# Observability Settings - Jaeger & MLflow
# =============================================================================
environment_variables:
  OTEL_EXPORTER_OTLP_ENDPOINT: "http://jaeger:4318"
  OTEL_SERVICE_NAME: "litellm-gateway"
  OTEL_TRACES_EXPORTER: "otlp"
  MLFLOW_TRACKING_URI: "http://mlflow:5050"

# =============================================================================
# MCP Servers Configuration
# =============================================================================
# MCP servers can be configured via API when MCP_GATEWAY_ENABLED=true
# The following servers are available via stdio transport in the mcp-proxy container:
#
# Available MCP Servers (stdio-based, no API keys required):
# - @anthropic-ai/server-filesystem: File system access for reading workspace files
# - @anthropic-ai/server-memory: Simple key-value memory for agents
#
# To add HTTP-based MCP servers dynamically:
# POST /mcp/servers with body:
# {
#   "server_id": "my-mcp-server",
#   "name": "My MCP Server",
#   "url": "http://localhost:8080/mcp",
#   "transport": "streamable_http"
# }

# =============================================================================
# A2A Agents - Test Configuration
# =============================================================================
# A2A agents are managed via API when A2A_GATEWAY_ENABLED=true
# Example agents that can be added via API:
# POST /a2a/agents with body:
# {
#   "agent_id": "my-agent",
#   "name": "My Agent",
#   "description": "A test agent",
#   "url": "http://localhost:9000/a2a",
#   "capabilities": ["chat", "code"]
# }

# =============================================================================
# Service Endpoints Reference (for testing)
# =============================================================================
# LiteLLM Gateway:     http://localhost:4010
# Jaeger UI:           http://localhost:16686
# MLflow UI:           http://localhost:5050
# MinIO Console:       http://localhost:9001 (admin: minioadmin/minioadmin)
# MinIO API:           http://localhost:9000
# MCP Proxy:           http://localhost:3100-3103 (for MCP server access)
