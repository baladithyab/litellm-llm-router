"""
Property-Based Tests for OpenTelemetry Observability Integration.

These tests validate the correctness properties defined in the design document
for observability and tracing (Requirements 6.x, 13.5, 14.6, 15.x).

Property tests use Hypothesis to generate many test cases and verify that
universal properties hold across all valid inputs.
"""

import os
from typing import Any, Dict, List
from unittest.mock import patch

from hypothesis import given, settings, strategies as st, assume, HealthCheck
from opentelemetry.sdk.trace import TracerProvider, ReadableSpan
from opentelemetry.sdk.trace.export import (
    SimpleSpanProcessor,
    SpanExporter,
    SpanExportResult,
)

# NOTE: We don't mock modules at the top level because it pollutes sys.modules
# and corrupts other tests that run later in the suite.
# The observability module is loaded directly via importlib.util to avoid
# importing the full litellm_llmrouter package which has heavy dependencies.

# Import the module under test directly
import importlib.util  # noqa: E402

spec = importlib.util.spec_from_file_location(
    "observability", "src/litellm_llmrouter/observability.py"
)
observability = importlib.util.module_from_spec(spec)
spec.loader.exec_module(observability)

ObservabilityManager = observability.ObservabilityManager
init_observability = observability.init_observability


# Test Data Generators
strategy_name_strategy = st.sampled_from(
    [
        "llmrouter-knn",
        "llmrouter-svm",
        "llmrouter-mlp",
        "llmrouter-elo",
        "simple-shuffle",
        "least-busy",
        "latency-based-routing",
    ]
)

model_name_strategy = st.text(
    alphabet=st.characters(whitelist_categories=("L", "N"), whitelist_characters="-_."),
    min_size=1,
    max_size=50,
).filter(lambda x: x.strip() and not x.startswith("-"))

cache_key_strategy = st.text(
    alphabet=st.characters(
        whitelist_categories=("L", "N"), whitelist_characters="-_.:"
    ),
    min_size=1,
    max_size=100,
)

cache_operation_strategy = st.sampled_from(["lookup", "set", "delete"])

latency_ms_strategy = st.floats(
    min_value=0.1, max_value=10000.0, allow_nan=False, allow_infinity=False
)

model_count_strategy = st.integers(min_value=1, max_value=100)


@st.composite
def routing_context_strategy(draw):
    """Generate routing context data."""
    return {
        "strategy": draw(strategy_name_strategy),
        "selected_model": draw(model_name_strategy),
        "model_count": draw(model_count_strategy),
        "latency_ms": draw(latency_ms_strategy),
    }


@st.composite
def error_context_strategy(draw):
    """Generate error context data."""
    return {
        "request_id": f"req-{draw(st.integers(min_value=1, max_value=10000))}",
        "user_id": f"user-{draw(st.integers(min_value=1, max_value=1000))}",
        "model": draw(model_name_strategy),
        "error_type": draw(
            st.sampled_from(["ValueError", "TimeoutError", "ConnectionError"])
        ),
    }


class InMemorySpanExporter(SpanExporter):
    """In-memory span exporter for testing."""

    def __init__(self):
        self.spans: List[ReadableSpan] = []

    def export(self, spans: List[ReadableSpan]) -> SpanExportResult:
        self.spans.extend(spans)
        return SpanExportResult.SUCCESS

    def shutdown(self) -> None:
        pass

    def force_flush(self, timeout_millis: int = 30000) -> bool:
        return True

    def clear(self):
        self.spans.clear()


class TestObservabilitySpanAndLogEmissionProperty:
    """
    Property 15: Observability Span and Log Emission

    For any request processed by the Gateway when observability is configured,
    the system should emit OpenTelemetry spans for all key events (routing
    decision, LLM call, cache hit/miss), emit structured logs with trace
    correlation IDs, and update metrics (request count, latency, error rate, cost).

    **Validates: Requirements 6.3, 6.4, 6.5, 6.9, 13.5, 14.6, 15.2, 15.4, 15.5**
    """

    @settings(max_examples=100, suppress_health_check=[HealthCheck.too_slow])
    @given(
        strategy=strategy_name_strategy,
        model_count=model_count_strategy,
    )
    def test_routing_span_contains_required_attributes(
        self, strategy: str, model_count: int
    ):
        """
        Property 15: Routing spans contain strategy and model count attributes.

        For any routing decision, the emitted span should include the strategy
        name and model count as attributes.
        """
        # Setup in-memory exporter with fresh tracer provider
        exporter = InMemorySpanExporter()
        tracer_provider = TracerProvider()
        tracer_provider.add_span_processor(SimpleSpanProcessor(exporter))

        # Get tracer directly from provider (don't set global)
        tracer = tracer_provider.get_tracer(__name__)

        # Create span directly using tracer
        with tracer.start_as_current_span("llm.routing.decision") as span:
            span.set_attribute("llm.routing.strategy", strategy)
            span.set_attribute("llm.routing.model_count", model_count)

        # Force flush
        tracer_provider.force_flush()

        # Verify span was emitted
        assert len(exporter.spans) > 0

        # Find the routing span
        routing_spans = [s for s in exporter.spans if s.name == "llm.routing.decision"]
        assert len(routing_spans) > 0

        routing_span = routing_spans[0]
        attrs = routing_span.attributes

        # Verify required attributes
        assert "llm.routing.strategy" in attrs
        assert attrs["llm.routing.strategy"] == strategy
        assert "llm.routing.model_count" in attrs
        assert attrs["llm.routing.model_count"] == model_count

    @settings(max_examples=100, suppress_health_check=[HealthCheck.too_slow])
    @given(
        operation=cache_operation_strategy,
        cache_key=cache_key_strategy,
    )
    def test_cache_span_contains_required_attributes(
        self, operation: str, cache_key: str
    ):
        """
        Property 15: Cache spans contain operation and key attributes.

        For any cache operation, the emitted span should include the operation
        type and cache key (truncated for privacy).
        """
        # Setup in-memory exporter with fresh tracer provider
        exporter = InMemorySpanExporter()
        tracer_provider = TracerProvider()
        tracer_provider.add_span_processor(SimpleSpanProcessor(exporter))

        # Get tracer directly from provider
        tracer = tracer_provider.get_tracer(__name__)

        # Create cache span directly
        with tracer.start_as_current_span(f"cache.{operation}") as span:
            # Truncate cache key for privacy (same as ObservabilityManager)
            span.set_attribute("cache.key", cache_key[:50] if cache_key else "")

        # Force flush
        tracer_provider.force_flush()

        # Verify span was emitted
        assert len(exporter.spans) > 0

        # Find the cache span
        cache_spans = [s for s in exporter.spans if s.name == f"cache.{operation}"]
        assert len(cache_spans) > 0

        cache_span = cache_spans[0]
        attrs = cache_span.attributes

        # Verify required attributes
        assert "cache.key" in attrs
        # Key should be truncated to 50 chars for privacy
        assert len(attrs["cache.key"]) <= 50

    @settings(max_examples=100, suppress_health_check=[HealthCheck.too_slow])
    @given(routing_ctx=routing_context_strategy())
    def test_routing_decision_logging_includes_context(
        self, routing_ctx: Dict[str, Any]
    ):
        """
        Property 15: Routing decision logs include strategy and selected model.

        For any routing decision, the log entry should include the strategy
        used and the model selected.
        """
        # Create manager
        manager = ObservabilityManager(
            service_name="test-service",
            enable_traces=False,
            enable_logs=False,  # Don't initialize OTLP to avoid connection
            enable_metrics=False,
        )

        # Capture log output
        with patch("logging.Logger.info") as mock_log:
            manager.log_routing_decision(
                strategy=routing_ctx["strategy"],
                selected_model=routing_ctx["selected_model"],
                latency_ms=routing_ctx["latency_ms"],
            )

            # Verify log was called
            assert mock_log.called

            # Get the log call
            call_args = mock_log.call_args

            # Verify message
            assert "Routing decision made" in call_args[0][0]

            # Verify extra context
            extra = call_args[1].get("extra", {})
            assert extra["event"] == "routing.decision"
            assert extra["strategy"] == routing_ctx["strategy"]
            assert extra["selected_model"] == routing_ctx["selected_model"]
            assert extra["latency_ms"] == routing_ctx["latency_ms"]

    @settings(max_examples=100, suppress_health_check=[HealthCheck.too_slow])
    @given(error_ctx=error_context_strategy())
    def test_error_logging_includes_trace_context(self, error_ctx: Dict[str, Any]):
        """
        Property 15: Error logs include trace context and error details.

        For any error, the log entry should include the error type, message,
        and additional context.
        """
        # Create manager
        manager = ObservabilityManager(
            service_name="test-service",
            enable_traces=False,
            enable_logs=False,
            enable_metrics=False,
        )

        # Create an error - use the error_type from context
        error_type = error_ctx["error_type"]
        if error_type == "ValueError":
            error = ValueError("Test error message")
        elif error_type == "TimeoutError":
            error = TimeoutError("Test error message")
        else:  # ConnectionError
            error = ConnectionError("Test error message")

        # Capture log output
        with patch("logging.Logger.error") as mock_log:
            manager.log_error_with_trace(error, context=error_ctx)

            # Verify log was called
            assert mock_log.called

            # Get the log call
            call_args = mock_log.call_args

            # Verify message contains error
            assert "Error occurred" in call_args[0][0]
            assert "Test error message" in call_args[0][0]

            # Verify extra context
            extra = call_args[1].get("extra", {})
            assert extra["event"] == "error"
            assert extra["error_type"] == error_type
            assert extra["error_message"] == "Test error message"
            assert extra["request_id"] == error_ctx["request_id"]
            assert extra["user_id"] == error_ctx["user_id"]
            assert extra["model"] == error_ctx["model"]

            # Verify exc_info is True for stack trace
            assert call_args[1].get("exc_info") is True

    @settings(max_examples=50, suppress_health_check=[HealthCheck.too_slow])
    @given(
        strategy=strategy_name_strategy,
        model_count=model_count_strategy,
    )
    def test_span_lifecycle_is_complete(self, strategy: str, model_count: int):
        """
        Property 15: Spans have complete lifecycle (start, attributes, end).

        For any span created, it should have a start time, end time, and
        all required attributes set.
        """
        # Setup in-memory exporter with fresh tracer provider
        exporter = InMemorySpanExporter()
        tracer_provider = TracerProvider()
        tracer_provider.add_span_processor(SimpleSpanProcessor(exporter))

        # Get tracer directly from provider
        tracer = tracer_provider.get_tracer(__name__)

        # Create and end span
        with tracer.start_as_current_span("llm.routing.decision") as span:
            span.set_attribute("llm.routing.strategy", strategy)
            span.set_attribute("llm.routing.model_count", model_count)

        # Force flush
        tracer_provider.force_flush()

        # Verify span
        assert len(exporter.spans) > 0
        exported_span = exporter.spans[0]

        # Verify lifecycle
        assert exported_span.start_time is not None
        assert exported_span.end_time is not None
        assert exported_span.end_time >= exported_span.start_time

        # Verify attributes are set
        assert exported_span.attributes is not None
        assert len(exported_span.attributes) > 0

    @settings(max_examples=50, suppress_health_check=[HealthCheck.too_slow])
    @given(
        cache_key=cache_key_strategy,
    )
    def test_cache_key_truncation_for_privacy(self, cache_key: str):
        """
        Property 15: Cache keys are truncated to 50 chars for privacy.

        For any cache key, the span attribute should contain at most 50
        characters to avoid logging sensitive data.
        """
        # Setup in-memory exporter with fresh tracer provider
        exporter = InMemorySpanExporter()
        tracer_provider = TracerProvider()
        tracer_provider.add_span_processor(SimpleSpanProcessor(exporter))

        # Get tracer directly from provider
        tracer = tracer_provider.get_tracer(__name__)

        # Create cache span with truncated key
        with tracer.start_as_current_span("cache.lookup") as span:
            span.set_attribute("cache.key", cache_key[:50] if cache_key else "")

        # Force flush
        tracer_provider.force_flush()

        # Verify truncation
        assert len(exporter.spans) > 0
        cache_span = exporter.spans[0]

        truncated_key = cache_span.attributes.get("cache.key", "")
        assert len(truncated_key) <= 50

        # If original key was longer, verify it was truncated
        if len(cache_key) > 50:
            assert truncated_key == cache_key[:50]
        else:
            assert truncated_key == cache_key

    @settings(max_examples=50, suppress_health_check=[HealthCheck.too_slow])
    @given(
        strategy=strategy_name_strategy,
        selected_model=model_name_strategy,
    )
    def test_logging_does_not_include_query_by_default(
        self, strategy: str, selected_model: str
    ):
        """
        Property 15: Query content is not logged by default for privacy.

        For any routing decision, the query content should not be logged
        unless explicitly enabled via LOG_QUERIES environment variable.
        """
        # Create manager
        manager = ObservabilityManager(
            service_name="test-service",
            enable_traces=False,
            enable_logs=False,
            enable_metrics=False,
        )

        query = "This is a sensitive user query"

        # Ensure LOG_QUERIES is not set
        with patch.dict(os.environ, {}, clear=True):
            with patch("logging.Logger.info") as mock_log:
                manager.log_routing_decision(
                    strategy=strategy,
                    selected_model=selected_model,
                    query=query,
                )

                # Verify log was called
                assert mock_log.called

                # Get extra context
                extra = mock_log.call_args[1].get("extra", {})

                # Query content should not be in extra
                assert "query" not in extra

                # Only query length should be present (if LOG_QUERIES was true)
                # But since it's false, even query_length shouldn't be there
                # Actually, looking at the code, query_length is only added if LOG_QUERIES is true
                assert "query_length" not in extra

    @settings(max_examples=30, suppress_health_check=[HealthCheck.too_slow])
    @given(
        strategy=strategy_name_strategy,
        model_count=model_count_strategy,
    )
    def test_multiple_spans_are_independent(self, strategy: str, model_count: int):
        """
        Property 15: Multiple spans can be created independently.

        For any sequence of operations, each span should be independent
        and not interfere with others.
        """
        # Setup in-memory exporter with fresh tracer provider
        exporter = InMemorySpanExporter()
        tracer_provider = TracerProvider()
        tracer_provider.add_span_processor(SimpleSpanProcessor(exporter))

        # Get tracer directly from provider
        tracer = tracer_provider.get_tracer(__name__)

        # Create multiple spans
        with tracer.start_as_current_span("llm.routing.decision") as span1:
            span1.set_attribute("llm.routing.strategy", strategy)
            span1.set_attribute("llm.routing.model_count", model_count)

        with tracer.start_as_current_span("cache.lookup") as span2:
            span2.set_attribute("cache.key", "key1")

        with tracer.start_as_current_span("cache.set") as span3:
            span3.set_attribute("cache.key", "key2")

        # Force flush
        tracer_provider.force_flush()

        # Verify all spans were emitted
        assert len(exporter.spans) >= 3

        # Verify each span has unique span_id
        span_ids = [s.context.span_id for s in exporter.spans]
        assert len(span_ids) == len(set(span_ids))

    def test_observability_manager_initialization_sets_resource_attributes(self):
        """
        Property 15: ObservabilityManager sets required resource attributes.

        For any initialized manager, the resource should contain service.name,
        service.version, deployment.environment, and service.namespace.
        """
        manager = ObservabilityManager(
            service_name="test-gateway",
            service_version="2.0.0",
            deployment_environment="staging",
        )

        attrs = manager.resource.attributes

        assert attrs["service.name"] == "test-gateway"
        assert attrs["service.version"] == "2.0.0"
        assert attrs["deployment.environment"] == "staging"
        assert attrs["service.namespace"] == "ai-gateway"

    @settings(max_examples=50, suppress_health_check=[HealthCheck.too_slow])
    @given(
        service_name=st.text(min_size=1, max_size=50).filter(lambda x: x.strip()),
        service_version=st.text(min_size=1, max_size=20).filter(lambda x: x.strip()),
    )
    def test_resource_attributes_are_consistent(
        self, service_name: str, service_version: str
    ):
        """
        Property 15: Resource attributes remain consistent across operations.

        For any service configuration, the resource attributes should remain
        the same throughout the manager's lifetime.
        """
        manager = ObservabilityManager(
            service_name=service_name,
            service_version=service_version,
        )

        # Get attributes multiple times
        attrs1 = manager.resource.attributes
        attrs2 = manager.resource.attributes

        # Should be the same object/values
        assert attrs1 == attrs2
        assert attrs1["service.name"] == service_name
        assert attrs2["service.name"] == service_name


class TestObservabilityConfiguration:
    """Additional tests for observability configuration properties."""

    @settings(max_examples=50, suppress_health_check=[HealthCheck.too_slow])
    @given(
        enable_traces=st.booleans(),
        enable_logs=st.booleans(),
        enable_metrics=st.booleans(),
    )
    def test_enable_flags_control_initialization(
        self, enable_traces: bool, enable_logs: bool, enable_metrics: bool
    ):
        """
        Property 15: Enable flags correctly control what gets initialized.

        For any combination of enable flags, only the enabled components
        should be initialized.
        """
        manager = ObservabilityManager(
            enable_traces=enable_traces,
            enable_logs=enable_logs,
            enable_metrics=enable_metrics,
        )

        assert manager.enable_traces == enable_traces
        assert manager.enable_logs == enable_logs
        assert manager.enable_metrics == enable_metrics

    @settings(max_examples=50, suppress_health_check=[HealthCheck.too_slow])
    @given(
        protocol=st.sampled_from(["http://", "https://"]),
        host=st.text(
            min_size=5,
            max_size=30,
            alphabet=st.characters(
                whitelist_categories=("L", "N"), whitelist_characters=".-"
            ),
        ).filter(lambda x: x and not x.startswith(".") and not x.endswith(".")),
        port=st.integers(min_value=1000, max_value=65535),
    )
    def test_otlp_endpoint_configuration(self, protocol: str, host: str, port: int):
        """
        Property 15: OTLP endpoint can be configured.

        For any valid OTLP endpoint URL, the manager should accept and
        store the configuration.
        """
        otlp_endpoint = f"{protocol}{host}:{port}"
        manager = ObservabilityManager(otlp_endpoint=otlp_endpoint)

        assert manager.otlp_endpoint == otlp_endpoint

    def test_default_otlp_endpoint_from_environment(self):
        """
        Property 15: OTLP endpoint defaults to environment variable.

        When no endpoint is provided, the manager should use the value
        from OTEL_EXPORTER_OTLP_ENDPOINT environment variable.
        """
        test_endpoint = "http://test-collector:4317"

        with patch.dict(os.environ, {"OTEL_EXPORTER_OTLP_ENDPOINT": test_endpoint}):
            manager = ObservabilityManager()
            assert manager.otlp_endpoint == test_endpoint

    def test_default_otlp_endpoint_fallback(self):
        """
        Property 15: OTLP endpoint has a default fallback value.

        When no endpoint is provided and no environment variable is set,
        the manager should use a default localhost endpoint.
        """
        with patch.dict(os.environ, {}, clear=True):
            manager = ObservabilityManager()
            assert manager.otlp_endpoint == "http://localhost:4317"


class TestPerTeamObservabilityProperty:
    """
    Property 16: Per-Team Observability Settings

    For any team configured in default_team_settings with specific observability
    callbacks, requests made with that team's virtual keys should send traces,
    logs, and metrics to the team-specific observability backend (e.g.,
    team-specific Langfuse project) with proper trace correlation.

    **Validates: Requirements 6.8**
    """

    @st.composite
    def team_config_strategy(draw):
        """Generate team configuration with observability settings."""
        team_id = f"team-{draw(st.integers(min_value=1, max_value=100))}"
        return {
            "team_id": team_id,
            "success_callback": draw(
                st.lists(
                    st.sampled_from(["langfuse", "prometheus", "otel"]),
                    min_size=1,
                    max_size=3,
                    unique=True,
                )
            ),
            "langfuse_project": f"{team_id}-project",
            "langfuse_public_key": f"pk-{team_id}",
            "langfuse_secret_key": f"sk-{team_id}",
        }

    @settings(max_examples=100, suppress_health_check=[HealthCheck.too_slow])
    @given(team_config=team_config_strategy())
    def test_team_config_has_required_fields(self, team_config: Dict[str, Any]):
        """
        Property 16: Team configuration contains required observability fields.

        For any team configuration, it should have team_id and success_callback.
        """
        assert "team_id" in team_config
        assert "success_callback" in team_config
        assert isinstance(team_config["success_callback"], list)
        assert len(team_config["success_callback"]) > 0

    @settings(max_examples=100, suppress_health_check=[HealthCheck.too_slow])
    @given(team_config=team_config_strategy())
    def test_team_observability_callbacks_are_valid(self, team_config: Dict[str, Any]):
        """
        Property 16: Team observability callbacks are from valid set.

        For any team configuration, all callbacks should be recognized
        observability backends.
        """
        valid_callbacks = {"langfuse", "prometheus", "otel", "datadog", "newrelic"}

        for callback in team_config["success_callback"]:
            assert callback in valid_callbacks

    @settings(max_examples=50, suppress_health_check=[HealthCheck.too_slow])
    @given(team_config=team_config_strategy())
    def test_langfuse_config_when_langfuse_enabled(self, team_config: Dict[str, Any]):
        """
        Property 16: Langfuse configuration is present when Langfuse is enabled.

        For any team with 'langfuse' in success_callback, the configuration
        should include langfuse_project, langfuse_public_key, and
        langfuse_secret_key.
        """
        if "langfuse" in team_config["success_callback"]:
            assert "langfuse_project" in team_config
            assert "langfuse_public_key" in team_config
            assert "langfuse_secret_key" in team_config

            # Verify format
            assert team_config["langfuse_project"].endswith("-project")
            assert team_config["langfuse_public_key"].startswith("pk-")
            assert team_config["langfuse_secret_key"].startswith("sk-")

    @settings(max_examples=100, suppress_health_check=[HealthCheck.too_slow])
    @given(
        team_config=team_config_strategy(),
        request_id=st.text(min_size=5, max_size=50),
    )
    def test_team_context_propagation(
        self, team_config: Dict[str, Any], request_id: str
    ):
        """
        Property 16: Team context is propagated through observability.

        For any request with a team_id, the observability context should
        include the team_id for proper filtering and routing.
        """
        # Simulate request context
        request_context = {
            "request_id": request_id,
            "team_id": team_config["team_id"],
            "callbacks": team_config["success_callback"],
        }

        # Verify team_id is present
        assert request_context["team_id"] == team_config["team_id"]

        # Verify callbacks match team config
        assert request_context["callbacks"] == team_config["success_callback"]

    @settings(max_examples=50, suppress_health_check=[HealthCheck.too_slow])
    @given(
        team_configs=st.lists(
            team_config_strategy(),
            min_size=2,
            max_size=5,
        )
    )
    def test_multiple_teams_have_isolated_configs(
        self, team_configs: List[Dict[str, Any]]
    ):
        """
        Property 16: Multiple teams have isolated observability configs.

        For any set of teams, each team's observability configuration
        should be independent and not interfere with others.
        """
        # Ensure team_ids are unique
        team_ids = [config["team_id"] for config in team_configs]
        unique_team_ids = set(team_ids)

        # If we have duplicates, skip this test case
        assume(len(team_ids) == len(unique_team_ids))

        # Verify each team has its own config
        for i, config1 in enumerate(team_configs):
            for j, config2 in enumerate(team_configs):
                if i != j:
                    # Different teams should have different IDs
                    assert config1["team_id"] != config2["team_id"]

                    # If both use Langfuse, they should have different projects
                    if (
                        "langfuse" in config1["success_callback"]
                        and "langfuse" in config2["success_callback"]
                    ):
                        assert (
                            config1["langfuse_project"] != config2["langfuse_project"]
                        )

    @settings(max_examples=50, suppress_health_check=[HealthCheck.too_slow])
    @given(team_config=team_config_strategy())
    def test_team_config_serialization(self, team_config: Dict[str, Any]):
        """
        Property 16: Team configuration can be serialized and deserialized.

        For any team configuration, it should round-trip through JSON
        serialization without loss of data.
        """
        import json

        # Serialize
        json_str = json.dumps(team_config)

        # Deserialize
        loaded_config = json.loads(json_str)

        # Verify equality
        assert loaded_config == team_config
        assert loaded_config["team_id"] == team_config["team_id"]
        assert loaded_config["success_callback"] == team_config["success_callback"]

    def test_default_team_settings_structure(self):
        """
        Property 16: default_team_settings has expected structure.

        The default_team_settings configuration should be a list of
        team configurations, each with required fields.
        """
        # Example default_team_settings structure
        default_team_settings = [
            {
                "team_id": "team-1",
                "success_callback": ["langfuse"],
                "langfuse_project": "team-1-project",
                "langfuse_public_key": "pk-team-1",
                "langfuse_secret_key": "sk-team-1",
            },
            {
                "team_id": "team-2",
                "success_callback": ["prometheus", "otel"],
            },
        ]

        # Verify structure
        assert isinstance(default_team_settings, list)
        assert len(default_team_settings) > 0

        for team_config in default_team_settings:
            assert "team_id" in team_config
            assert "success_callback" in team_config
            assert isinstance(team_config["success_callback"], list)

    @settings(max_examples=50, suppress_health_check=[HealthCheck.too_slow])
    @given(
        team_config=team_config_strategy(),
        trace_id=st.text(
            min_size=16,
            max_size=32,
            alphabet=st.characters(whitelist_categories=("L", "N")),
        ),
    )
    def test_trace_correlation_includes_team_id(
        self, team_config: Dict[str, Any], trace_id: str
    ):
        """
        Property 16: Trace correlation includes team_id for filtering.

        For any trace generated for a team request, the trace context
        should include the team_id to enable team-specific filtering
        in observability backends.
        """
        # Simulate trace context
        trace_context = {
            "trace_id": trace_id,
            "team_id": team_config["team_id"],
            "callbacks": team_config["success_callback"],
        }

        # Verify team_id is in trace context
        assert "team_id" in trace_context
        assert trace_context["team_id"] == team_config["team_id"]

        # Verify trace_id is present
        assert "trace_id" in trace_context
        assert len(trace_context["trace_id"]) >= 16

    @settings(max_examples=50, suppress_health_check=[HealthCheck.too_slow])
    @given(team_config=team_config_strategy())
    def test_team_observability_backend_routing(self, team_config: Dict[str, Any]):
        """
        Property 16: Observability data is routed to team-specific backends.

        For any team configuration, the observability data should be
        routed to the backends specified in success_callback.
        """
        # Simulate routing logic
        enabled_backends = set(team_config["success_callback"])

        # Verify routing
        if "langfuse" in enabled_backends:
            assert "langfuse_project" in team_config

        if "prometheus" in enabled_backends:
            # Prometheus is always available
            assert "prometheus" in enabled_backends

        if "otel" in enabled_backends:
            # OTEL is always available
            assert "otel" in enabled_backends

    @settings(max_examples=30, suppress_health_check=[HealthCheck.too_slow])
    @given(
        team_config=team_config_strategy(),
        num_requests=st.integers(min_value=1, max_value=100),
    )
    def test_team_metrics_aggregation(
        self, team_config: Dict[str, Any], num_requests: int
    ):
        """
        Property 16: Team metrics can be aggregated per team.

        For any team and number of requests, metrics should be
        aggregatable by team_id for per-team analytics.
        """
        # Simulate metrics collection
        metrics = []
        for i in range(num_requests):
            metrics.append(
                {
                    "team_id": team_config["team_id"],
                    "request_count": 1,
                    "latency_ms": 100 + i,
                }
            )

        # Aggregate by team
        team_metrics = {}
        for metric in metrics:
            team_id = metric["team_id"]
            if team_id not in team_metrics:
                team_metrics[team_id] = {
                    "request_count": 0,
                    "total_latency": 0,
                }
            team_metrics[team_id]["request_count"] += metric["request_count"]
            team_metrics[team_id]["total_latency"] += metric["latency_ms"]

        # Verify aggregation
        assert team_config["team_id"] in team_metrics
        assert team_metrics[team_config["team_id"]]["request_count"] == num_requests
        assert team_metrics[team_config["team_id"]]["total_latency"] > 0
