# =============================================================================
# LiteLLM + LLMRouter - Local Testing Setup
# =============================================================================
# This docker-compose file simulates a production AWS environment locally.
#
# MANDATORY COMPONENT:
#   - litellm-gateway: The core LLM proxy/gateway (required)
#
# OPTIONAL COMPONENTS (simulating AWS services):
#   - postgres:  Simulates Amazon RDS / Aurora PostgreSQL
#   - redis:     Simulates ElastiCache Redis / Valkey / MemoryDB
#   - minio:     Simulates Amazon S3
#   - jaeger:    Simulates AWS X-Ray / CloudWatch (OTEL tracing)
#   - mlflow:    Simulates SageMaker / self-hosted experiment tracking
#
# For minimal testing, you can run just the gateway:
#   docker compose -f docker-compose.local-test.yml up litellm-gateway
#
# For full testing with all services:
#   docker compose -f docker-compose.local-test.yml up
# =============================================================================

services:
  # ==========================================================================
  # OPTIONAL: PostgreSQL - Simulates Amazon RDS / Aurora PostgreSQL
  # AWS Alternative: Amazon RDS, Aurora PostgreSQL, Aurora Serverless v2
  # Purpose: API key management, spend tracking, team management
  # ==========================================================================
  postgres:
    image: postgres:16-alpine
    container_name: litellm-test-postgres
    environment:
      POSTGRES_USER: litellm
      POSTGRES_PASSWORD: testpassword
      POSTGRES_DB: litellm
    volumes:
      - test_postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U litellm"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - litellm-test-network

  # ==========================================================================
  # OPTIONAL: Redis - Simulates ElastiCache Redis / Valkey / MemoryDB
  # AWS Alternative: ElastiCache Redis, ElastiCache Valkey, MemoryDB
  # Purpose: Response caching, rate limiting, distributed state
  # ==========================================================================
  redis:
    image: redis:7-alpine
    container_name: litellm-test-redis
    command: redis-server --appendonly yes
    volumes:
      - test_redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - litellm-test-network

  # ==========================================================================
  # OPTIONAL: Jaeger - Simulates AWS X-Ray / CloudWatch Tracing
  # AWS Alternative: AWS X-Ray, CloudWatch Container Insights, Managed Grafana
  # Purpose: Distributed tracing, request tracking, performance analysis
  # ==========================================================================
  jaeger:
    image: jaegertracing/all-in-one:1.54
    container_name: litellm-test-jaeger
    ports:
      - "16686:16686"  # Jaeger UI
      - "4317:4317"    # OTLP gRPC
      - "4318:4318"    # OTLP HTTP
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    networks:
      - litellm-test-network

  # ==========================================================================
  # OPTIONAL: MinIO - Simulates Amazon S3
  # AWS Alternative: Amazon S3
  # Purpose: Configuration files, ML models, hot-reload support
  # ==========================================================================
  minio:
    image: minio/minio:latest
    container_name: litellm-test-minio
    ports:
      - "9000:9000"    # MinIO API
      - "9001:9001"    # MinIO Console
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - test_minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - litellm-test-network

  # MinIO bucket initialization
  minio-init:
    image: minio/mc:latest
    container_name: litellm-test-minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set myminio http://minio:9000 minioadmin minioadmin;
      mc mb myminio/mlflow --ignore-existing;
      mc mb myminio/llmrouter-models --ignore-existing;
      mc mb myminio/litellm-config --ignore-existing;
      mc anonymous set download myminio/mlflow;
      exit 0;
      "
    networks:
      - litellm-test-network

  # ==========================================================================
  # OPTIONAL: MLflow - Simulates SageMaker / ML Experiment Tracking
  # AWS Alternative: SageMaker Experiments, self-hosted on ECS, S3 + DynamoDB
  # Purpose: LLMRouter model training, experiment tracking, model versioning
  # ==========================================================================
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.16.0
    container_name: litellm-test-mlflow
    depends_on:
      minio:
        condition: service_healthy
    ports:
      - "5050:5050"
    environment:
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
    volumes:
      - test_mlflow_data:/mlflow
    command: >
      mlflow server
      --backend-store-uri sqlite:///mlflow/mlflow.db
      --artifacts-destination s3://mlflow/artifacts
      --host 0.0.0.0
      --port 5050
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:5050/health')\""]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - litellm-test-network

  # ==========================================================================
  # MANDATORY: LiteLLM + LLMRouter Gateway - Core LLM Proxy
  # AWS Deployment: ECS Fargate, EKS, App Runner, Lambda
  # Purpose: LLM request routing, API proxy, load balancing
  # ==========================================================================
  litellm-gateway:
    build:
      context: .
      dockerfile: docker/Dockerfile.local
    container_name: litellm-test-gateway
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      jaeger:
        condition: service_started
      mlflow:
        condition: service_healthy
    ports:
      - "4010:4000"
    volumes:
      - ./config:/app/config:ro
      - ./models:/app/models:ro
      - ./custom_routers:/app/custom_routers:ro
    environment:
      # Core settings
      - LITELLM_CONFIG_PATH=/app/config/config.local-test.yaml
      - LITELLM_MASTER_KEY=sk-test-master-key
      # Admin API key for control-plane authentication (MCP, A2A, hot-reload management)
      # Using the same key as master for simplicity in local testing
      - ADMIN_API_KEY=sk-test-admin-key
      - ADMIN_API_KEYS=sk-test-admin-key,sk-test-master-key
      # Enable database for full feature testing
      - DATABASE_URL=postgresql://litellm:testpassword@postgres:5432/litellm
      # Run database migrations on startup (safe for single-node local testing)
      - LITELLM_RUN_DB_MIGRATIONS=true
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      # AWS Bedrock - Use EC2 instance metadata service (IMDSv2)
      # Container needs network access to host's metadata service
      - AWS_DEFAULT_REGION=us-east-1
      - AWS_EC2_METADATA_DISABLED=false
      # Gateway Features - ALL ENABLED for testing
      - A2A_GATEWAY_ENABLED=true
      - MCP_GATEWAY_ENABLED=true
      - LLMROUTER_ENABLE_MCP_TOOL_INVOCATION=true
      # Allow outbound requests to private IPs (Docker network) for MCP tool invocation
      - LLMROUTER_OUTBOUND_ALLOW_PRIVATE=true
      - STORE_MODEL_IN_DB=true
      # LLMRouter Settings
      - LLMROUTER_MODELS_PATH=/app/models
      - LLMROUTER_HOT_RELOAD=true
      - LLMROUTER_RELOAD_INTERVAL=30
      # Config Sync & Hot Reload
      - CONFIG_HOT_RELOAD=true
      - CONFIG_SYNC_ENABLED=true
      - CONFIG_SYNC_INTERVAL=30
      # OpenTelemetry / Jaeger Tracing
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:4318
      - OTEL_SERVICE_NAME=litellm-gateway
      - OTEL_TRACES_EXPORTER=otlp
      # MLflow Integration (uses MinIO, not AWS S3)
      - MLFLOW_TRACKING_URI=http://mlflow:5050
      # MinIO/S3 for config storage (MLflow uses its own env vars configured in mlflow container)
      - S3_ENDPOINT_URL=http://minio:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin
    # Allow container to access EC2 instance metadata service for AWS credentials
    extra_hosts:
      - "host.docker.internal:host-gateway"
    command: ["--config", "/app/config/config.local-test.yaml", "--port", "4000"]
    healthcheck:
      test: ["CMD", "curl", "-f", "-H", "Authorization: Bearer sk-test-master-key", "http://localhost:4000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - litellm-test-network

  # ==========================================================================
  # MCP Stub Server - Deterministic test server for MCP gateway validation
  # Purpose: Provides stub.echo, stub.sum tools for E2E validation
  # ==========================================================================
  mcp-stub-server:
    build:
      context: .
      dockerfile: docker/Dockerfile.local
    container_name: litellm-test-mcp-stub
    ports:
      - "9100:9100"
    volumes:
      - ./scripts:/app/scripts:ro
    environment:
      - MCP_STUB_NAME=Test Stub MCP Server
      - MCP_STUB_VERSION=0.1.0
      - MCP_STUB_PORT=9100
    # Override entrypoint to bypass gateway's litellm startup
    entrypoint: ["/usr/bin/tini", "--"]
    command: ["python", "/app/scripts/mcp_stub_server.py"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9100/mcp/health"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 10s
    networks:
      - litellm-test-network

  # ==========================================================================
  # MCP Servers - No API Key Required (stdio-over-HTTP bridge)
  # ==========================================================================
  # Note: MCP servers typically use stdio transport. For HTTP access, we use
  # mcp-proxy to bridge stdio servers to HTTP endpoints.

  # MCP Proxy - Bridges stdio MCP servers to HTTP
  # This allows HTTP-based access to any stdio MCP server
  mcp-proxy:
    image: node:20-alpine
    container_name: litellm-test-mcp-proxy
    working_dir: /app
    volumes:
      - ./:/workspace:ro
      - mcp_node_modules:/app/node_modules
    ports:
      - "3100:3100"  # Filesystem MCP
      - "3101:3101"  # Memory MCP
      - "3102:3102"  # Time MCP
      - "3103:3103"  # Echo MCP (test server)
    environment:
      - NODE_ENV=production
    command: >
      sh -c "
        npm install -g @anthropic-ai/server-filesystem @anthropic-ai/server-memory &&
        echo 'MCP servers installed. Running in stdio mode for local testing.'
        echo 'Use MCP client to connect via stdio transport.'
        tail -f /dev/null
      "
    networks:
      - litellm-test-network
    healthcheck:
      test: ["CMD", "echo", "healthy"]
      interval: 30s
      timeout: 10s
      retries: 3

# AWS Documentation MCP Server (uses public AWS docs, no key needed)
# This server is already available as part of the litellm-gateway via aws_knowledge_mcp tool

volumes:
  test_postgres_data:
  test_redis_data:
  test_minio_data:
  test_mlflow_data:
  mcp_node_modules:

networks:
  litellm-test-network:
    driver: bridge
