# LiteLLM + LLMRouter Environment Variables
# Copy this file to .env and fill in your values
#
# SECURITY: Generate secure keys with: openssl rand -hex 32
# NEVER commit real credentials to version control!
#
# For Kubernetes deployments, these values are typically provided via:
# - ConfigMaps (for non-sensitive config)
# - Secrets (for credentials)
# - External secret managers (e.g., AWS Secrets Manager, HashiCorp Vault)

# =============================================================================
# Required Settings (MUST be set before deployment)
# =============================================================================

# Master API key for admin access
# REQUIRED - Generate with: openssl rand -hex 32
# WARNING: This is a placeholder - DO NOT use in production!
LITELLM_MASTER_KEY=CHANGE_ME_GENERATE_WITH_openssl_rand_hex_32

# =============================================================================
# Admin API Keys for Control-Plane Operations
# =============================================================================
# These keys are required to access control-plane endpoints:
# - POST /router/reload, POST /config/reload (hot reload)
# - POST/PUT/DELETE /llmrouter/mcp/servers/* (MCP server management)
# - POST /llmrouter/mcp/tools/call (MCP tool invocation)
# - POST/DELETE /a2a/agents (A2A agent registration)
#
# Generate with: openssl rand -hex 32
# Comma-separated for multiple keys
ADMIN_API_KEYS=

# Legacy single-key support (prefer ADMIN_API_KEYS)
# ADMIN_API_KEY=

# Set to "false" to disable admin auth (NOT recommended for production)
# When disabled, control-plane endpoints fall back to user API key auth only
# Default: true (admin auth enabled)
# ADMIN_AUTH_ENABLED=true

# =============================================================================
# LLM Provider API Keys
# =============================================================================

# OpenAI (replace with your actual key)
OPENAI_API_KEY=

# Anthropic (replace with your actual key)
ANTHROPIC_API_KEY=

# Azure OpenAI
AZURE_API_KEY=
AZURE_API_BASE=

# Google Vertex AI
GOOGLE_APPLICATION_CREDENTIALS=

# AWS Bedrock (uses IAM roles in K8s via IRSA/Pod Identity)
# AWS_ACCESS_KEY_ID=
# AWS_SECRET_ACCESS_KEY=
AWS_DEFAULT_REGION=us-east-1

# =============================================================================
# Database (PostgreSQL) - Required for HA/K8s
# =============================================================================
# For Kubernetes, use a managed PostgreSQL service (RDS, Cloud SQL, etc.)
# or deploy a PostgreSQL StatefulSet with persistent volumes.

POSTGRES_USER=litellm
# REQUIRED for HA - Generate with: openssl rand -hex 16
# WARNING: This is a placeholder - DO NOT use in production!
POSTGRES_PASSWORD=CHANGE_ME_GENERATE_WITH_openssl_rand_hex_16
POSTGRES_DB=litellm

# Full connection string - typically constructed from above or provided by K8s Secret
# Format: postgresql://USER:PASSWORD@HOST:PORT/DATABASE
DATABASE_URL=postgresql://litellm:YOUR_POSTGRES_PASSWORD@postgres:5432/litellm

# Store LiteLLM models/config in database for multi-replica consistency
# Set to 'true' for Kubernetes deployments
STORE_MODEL_IN_DB=false

# =============================================================================
# Redis - Required for caching and rate limiting in HA/K8s
# =============================================================================
# For Kubernetes, use a managed Redis service (ElastiCache, MemoryStore, etc.)
# or deploy Redis with proper persistence.

REDIS_HOST=redis
REDIS_PORT=6379
# REDIS_PASSWORD=  # Uncomment if Redis requires authentication

# =============================================================================
# Object Storage Configuration Sync (S3/GCS)
# =============================================================================
# For K8s deployments, sync config from object storage instead of ConfigMaps
# for dynamic updates without pod restarts.

# Config from S3
CONFIG_S3_BUCKET=
CONFIG_S3_KEY=configs/config.yaml

# Config from GCS (alternative to S3)
CONFIG_GCS_BUCKET=
CONFIG_GCS_KEY=configs/config.yaml

# Hot reload and sync settings
CONFIG_HOT_RELOAD=false
CONFIG_SYNC_ENABLED=true
CONFIG_SYNC_INTERVAL=60

# =============================================================================
# Config Sync Leader Election (HA Deployments)
# =============================================================================
# In HA deployments with multiple replicas, leader election ensures only one
# replica performs config sync at a time, avoiding thundering herd and conflicts.
#
# Uses a database-backed lease lock (requires DATABASE_URL to be configured).
# Without DATABASE_URL, leader election is disabled and all replicas sync.

# Enable/disable leader election for config sync
# Default: true if DATABASE_URL is set (HA mode), false otherwise
# LLMROUTER_CONFIG_SYNC_LEADER_ELECTION_ENABLED=true

# Lease duration in seconds (how long a leader holds the lock)
# Default: 30
# LLMROUTER_CONFIG_SYNC_LEASE_SECONDS=30

# How often to renew the lease in seconds (should be < lease_seconds / 3)
# Default: 10
# LLMROUTER_CONFIG_SYNC_RENEW_INTERVAL_SECONDS=10

# Lock name (allows multiple independent locks if needed)
# Default: config_sync
# LLMROUTER_CONFIG_SYNC_LOCK_NAME=config_sync

# Models from S3 (for ML-based routing strategies)
LLMROUTER_MODEL_S3_BUCKET=
LLMROUTER_MODEL_S3_KEY=models/router.pt

# AWS Credentials for S3 (if not using IAM roles/IRSA)
# In K8s, prefer IRSA (EKS) or Workload Identity (GKE) over static credentials
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=

# =============================================================================
# Gateway Feature Flags
# =============================================================================

# Enable MCP (Model Context Protocol) gateway
MCP_GATEWAY_ENABLED=false

# Enable A2A (Agent-to-Agent) gateway
A2A_GATEWAY_ENABLED=false

# MCP HA sync via Redis (for multi-replica K8s deployments)
MCP_HA_SYNC_ENABLED=false

# Enable MCP remote tool invocation (SECURITY SENSITIVE)
# When false (default): POST /llmrouter/mcp/tools/call returns 501 Not Implemented
# When true: Gateway makes outbound HTTP calls to registered MCP servers
# CAUTION: Only enable in trusted environments with proper SSRF allowlists configured
LLMROUTER_ENABLE_MCP_TOOL_INVOCATION=false

# =============================================================================
# SSRF Protection (Outbound URL Security)
# =============================================================================
# Controls validation of outbound URLs for MCP servers and A2A agents.
# By default, private IPs are BLOCKED (secure-by-default / fail-closed).

# Allow all private IP ranges (10.x, 172.16.x, 192.168.x)
# Default: false (blocked) - set to true only for development/testing
LLMROUTER_ALLOW_PRIVATE_IPS=false

# Allowlist specific hosts/domains (comma-separated)
# Supports:
# - Exact match: "myserver.internal"
# - Suffix match: ".trusted.com" matches "api.trusted.com", "mcp.trusted.com"
# Example: LLMROUTER_SSRF_ALLOWLIST_HOSTS=mcp.internal,.trusted-corp.com
LLMROUTER_SSRF_ALLOWLIST_HOSTS=

# Allowlist specific IP ranges in CIDR notation (comma-separated)
# Use this to allow specific private IP ranges for internal services
# Example: LLMROUTER_SSRF_ALLOWLIST_CIDRS=10.100.0.0/16,192.168.50.0/24
LLMROUTER_SSRF_ALLOWLIST_CIDRS=

# =============================================================================
# Resilience / Backpressure Settings
# =============================================================================
# Controls gateway-level resilience primitives for load shedding and graceful shutdown.
# By default, backpressure is DISABLED (no concurrency limit) for non-breaking behavior.

# Maximum concurrent requests before returning 503 (load shedding)
# Default: 0 (disabled - no concurrency limit)
# When set: Requests over this limit receive HTTP 503 with JSON body:
#   {"error": "over_capacity", "message": "Server is at capacity, please retry later"}
# Health endpoints (/_health/live, /_health/ready) are always excluded from limiting.
# Example: ROUTEIQ_MAX_CONCURRENT_REQUESTS=100
ROUTEIQ_MAX_CONCURRENT_REQUESTS=0

# Drain timeout in seconds for graceful shutdown
# Default: 30
# On shutdown: Server waits up to this many seconds for in-flight requests to complete.
# After timeout, shutdown proceeds even if requests are still active.
# Example: ROUTEIQ_DRAIN_TIMEOUT_SECONDS=60
ROUTEIQ_DRAIN_TIMEOUT_SECONDS=30

# Additional paths to exclude from backpressure limiting (comma-separated)
# Default paths always excluded: /_health/live, /_health/ready, /health/*, etc.
# Example: ROUTEIQ_BACKPRESSURE_EXCLUDED_PATHS=/metrics,/internal/status
ROUTEIQ_BACKPRESSURE_EXCLUDED_PATHS=

# =============================================================================
# OpenTelemetry (OTEL) Observability
# =============================================================================
# For K8s, typically point to an OTEL Collector sidecar or service.

# OTEL Collector endpoint (gRPC)
# Examples:
#   - Sidecar: http://localhost:4317
#   - Service: http://otel-collector.observability:4317
OTEL_EXPORTER_OTLP_ENDPOINT=

# Service name for traces/metrics
OTEL_SERVICE_NAME=litellm-gateway

# Exporter configuration (set to 'otlp' when endpoint is configured)
OTEL_TRACES_EXPORTER=none
OTEL_METRICS_EXPORTER=none
OTEL_LOGS_EXPORTER=none

# Enable OTEL integration
OTEL_ENABLED=true

# -----------------------------------------------------------------------------
# Trace Sampling Configuration
# -----------------------------------------------------------------------------
# Control trace sampling to reduce volume in high-traffic production environments.
# Two approaches are supported:
#
# Option 1: Standard OTEL environment variables (recommended for compliance)
#   OTEL_TRACES_SAMPLER - Sampler type:
#     - "always_on": Sample all traces (default, 100%)
#     - "always_off": Sample no traces (0%)
#     - "traceidratio": Sample based on OTEL_TRACES_SAMPLER_ARG
#     - "parentbased_always_on": Parent-based with always_on root
#     - "parentbased_always_off": Parent-based with always_off root
#     - "parentbased_traceidratio": Parent-based with ratio root (recommended)
#   OTEL_TRACES_SAMPLER_ARG - Ratio for ratio-based samplers (0.0-1.0)
#
# Option 2: Simple rate-based sampling (convenience)
#   LLMROUTER_OTEL_SAMPLE_RATE - Sampling rate 0.0-1.0 (uses parentbased_traceidratio)
#
# Examples:
#   # Sample 10% of traces (high traffic production)
#   OTEL_TRACES_SAMPLER=parentbased_traceidratio
#   OTEL_TRACES_SAMPLER_ARG=0.1
#
#   # Or equivalently using the convenience variable:
#   LLMROUTER_OTEL_SAMPLE_RATE=0.1
#
#   # Sample all traces (development/debugging)
#   OTEL_TRACES_SAMPLER=always_on
#
OTEL_TRACES_SAMPLER=
OTEL_TRACES_SAMPLER_ARG=
LLMROUTER_OTEL_SAMPLE_RATE=

# -----------------------------------------------------------------------------
# Multiprocess Metrics (gunicorn/uvicorn workers)
# -----------------------------------------------------------------------------
# When running with multiple worker processes (gunicorn, uvicorn --workers),
# metrics require special handling to avoid duplicates or data loss.
#
# Recommended approach: Use an OTEL Collector as aggregation point
#   - Each worker sends metrics via OTLP to the collector
#   - Collector aggregates and exports to your backend (Prometheus, etc.)
#   - Set OTEL_EXPORTER_OTLP_ENDPOINT and ensure workers don't share state
#
# For Prometheus pull-based metrics in multiprocess mode:
#   - Set PROMETHEUS_MULTIPROC_DIR to a shared directory
#   - Mount tmpfs or emptyDir for performance
#   - Example: PROMETHEUS_MULTIPROC_DIR=/tmp/prometheus_multiproc
#
# See docs/observability.md for detailed multiprocess configuration guides.
#
# PROMETHEUS_MULTIPROC_DIR=

# =============================================================================
# LLMRouter Settings
# =============================================================================

LLMROUTER_HOT_RELOAD=true
LLMROUTER_RELOAD_INTERVAL=300

# =============================================================================
# MLOps (for training setup - not typically needed in production K8s)
# =============================================================================

MLFLOW_TRACKING_URI=http://mlflow:5000
MLFLOW_ARTIFACT_BUCKET=llmrouter-artifacts
WANDB_API_KEY=
HF_TOKEN=

# MinIO (local S3-compatible storage for development)
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin

# Jupyter
JUPYTER_TOKEN=llmrouter

# =============================================================================
# Kubernetes-Specific Notes
# =============================================================================
#
# Database Migrations:
#   - Do NOT run migrations on every replica (set LITELLM_RUN_DB_MIGRATIONS=false)
#   - Use a separate init container or Job for migrations
#   - Example: kubectl create job db-migrate --from=cronjob/litellm-migrate
#
# Health Probes:
#   - Liveness: /_health/live (no external deps, fast)
#   - Readiness: /_health/ready (checks DB/Redis if configured)
#   - LiteLLM native: /health/liveliness, /health/readiness (may be auth-protected)
#
# Network Policies:
#   - Egress to LLM providers (OpenAI, Anthropic, etc.)
#   - Egress to PostgreSQL and Redis
#   - Egress to S3/GCS for config sync
#   - Egress to MCP servers (if MCP_GATEWAY_ENABLED=true)
#   - Egress to A2A agent URLs (if A2A_GATEWAY_ENABLED=true)
#   - Egress to OTEL Collector
#
# Resource Recommendations:
#   - Memory: 512Mi-2Gi depending on traffic
#   - CPU: 500m-2000m depending on traffic
#   - Consider HPA based on CPU/memory or custom metrics
#
# Security Context (recommended):
#   securityContext:
#     runAsNonRoot: true
#     runAsUser: 1000
#     readOnlyRootFilesystem: true  # Requires tmpfs for /tmp
#     allowPrivilegeEscalation: false
# =============================================================================
